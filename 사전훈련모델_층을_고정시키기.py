# -*- coding: utf-8 -*-
"""사전훈련모델 층을 고정시키기.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-RMGhc0DkZraMKw4BlpaOK31gCTCx065
"""

import torch

!pip install transformers

from google.colab import drive
drive.mount('/content/gdriv')

#nsmc 다운로드
!git clone https://github.com/e9t/nsmc.git  #영화 댓글 데이터

import pandas as pd

train = pd.read_csv("nsmc/ratings_train.txt", sep='\t')
test = pd.read_csv("nsmc/ratings_test.txt", sep='\t')

print(train.shape)
print(test.shape)
## 리뷰 문장 추출
#sentences = train['document']
#라벨 추출
#labels = train['label'].values
train.drop(['id'],inplace=True,axis=1)
train.rename(columns={'document':'review','sentiment':'label'},inplace=True)

train_set=train[:1000]
valid_set=train[1000:1500]
test_set=train[1500:2000]

kb_albert_model_path='/content/gdriv/My Drive/Colab Notebooks/KB-albert/kb-albert-char-base-v2/kb-albert-char-base-v2'

'''필사한 건데 안 돌아ㅠㅠ

from transformers import AutoTokenizer

class TheDataset(torch.utils.data.Dataset):

  def __init__(self,reviews,sentiments,tokenizer):
    self.reviews=reviews
    self.sentiments=sentiments
    self.tokenizer=tokenizer
    self.max_len=tokenizer.model_max_length

  def __len__(self):
    return len(self.reviews)

  def __getitem__(self,index):
    review=str(self.reviews[index])
    sentiments=self.sentiments[index]

    encoded_review=self.tokenzier.encode_plus(
        review,
        add_special_tokens=True,
        max_length=self.max_len,
        return_token_type_ids=True,
        return_tensors='pt',
        padding='max_length',
        truncation=True
    )



    return{
        'input_ids':encoded_review['input_ids'][0],
        'attention_mask':encoded_review['attention_mask'][0],
        'labels':torch.tensor(sentiments,dtype=torch.long)
    }

tokenizer=AutoTokenizer.from_pretrained(kb_albert_model_path)

train_set_dataset=TheDataset(
    reviews=train_set.review.tolist(),
    #sentiments=train_set.vader_result.tolist(),  #vader 라는 비지도학습기반 라벨 부여
    sentiments=train_set.label.tolist(),
    tokenizer=tokenizer,
)

valid_set_dataset=TheDataset(
    reviews=valid_set.review.tolist(),
    #sentiments=valid_set.vader_result.tolist()
    sentiments=valid_set.label.tolist(),
    tokenizer=tokenizer,
)

train_set_dataloader=torch.utils.data.DataLoader(
    train_set_dataset,
    batch_size=16,
    num_workers=2   #권장되는 코어수
)

valid_set_dataloader=torch.utils.data.DataLoader(
    valid_set_dataset,
    batch_size=16,
    num_workers=2
)

train_data=next(iter(train_set_dataloader))
valid_data=next(iter(valid_set_dataloader)) #iter를 하나씩 next하면서 집어넣음

print(train_data['input_ids'].size(),valid_data['input_ids'].size())

'''

#ex1에 훈련모델 만들 때 사용한 코드
#input값인 train_set_dataset을 cuda에 올리지 않았음. 근데 왜 속도가 빨라졌지? 데이터로더에서 pin_memory=True 가 디폴트 값이어서 그런가?


from transformers import AutoTokenizer


# Create The Dataset Class.
class TheDataset(torch.utils.data.Dataset):

    def __init__(self, reviews, sentiments, tokenizer):
        self.reviews    = reviews
        self.sentiments = sentiments
        self.tokenizer  = tokenizer
        self.max_len    = tokenizer.model_max_length
  
    def __len__(self):
        return len(self.reviews)
  
    def __getitem__(self, index):
        review = str(self.reviews[index])
        sentiments = self.sentiments[index]

        encoded_review = self.tokenizer.encode_plus(
            review,
            add_special_tokens    = True,
            max_length            = self.max_len,
            return_token_type_ids = False,
            return_attention_mask = True,
            return_tensors        = "pt",
            padding               = "max_length",
            truncation            = True
        )

        return {
            'input_ids': encoded_review['input_ids'][0],
            'attention_mask': encoded_review['attention_mask'][0],
            'labels': torch.tensor(sentiments, dtype=torch.long)
        }

# Load the tokenizer for the BERT model.
tokenizer=AutoTokenizer.from_pretrained(kb_albert_model_path)

# Create Dataset objects for train/validation sets.
train_set_dataset = TheDataset(
    reviews    = train_set.review.tolist(),
    sentiments = train_set.label.tolist(),
    tokenizer  = tokenizer,
)

valid_set_dataset = TheDataset(
    reviews    = valid_set.review.tolist(),
    sentiments = valid_set.label.tolist(),
    tokenizer  = tokenizer,
)
'''
# Create DataLoader for train/validation sets.
train_set_dataloader = torch.utils.data.DataLoader(
    train_set_dataset,
    batch_size  = 16,
    num_workers = 2,
)

valid_set_dataloader = torch.utils.data.DataLoader(
    valid_set_dataset,
    batch_size  = 16,
    num_workers = 2,  #2 권장
)




# Get one batch as example.
train_data = next(iter(train_set_dataloader))
valid_data = next(iter(valid_set_dataloader))

train_data['attention_mask']=train_data['attention_mask'].to(device)
train_data['input_ids']=train_data['input_ids'].to(device)
train_data['labels']=train_data['labels'].to(device)
valid_data['attention_mask']=valid_data['attention_mask'].to(device)
valid_data['input_ids']=valid_data['input_ids'].to(device)
valid_data['labels']=valid_data['labels'].to(device)

# Print the output sizes.
print( train_data["input_ids"].size(), valid_data["input_ids"].size() )
'''

from transformers import BertForSequenceClassification
model=BertForSequenceClassification.from_pretrained(kb_albert_model_path)

device = "cuda" if torch.cuda.is_available() else "cpu"

device

"""파인튜닝 위해서 모델 구조비교"""

model = BertForSequenceClassification.from_pretrained("bert-large-uncased")

model

for name, param in model.bert.named_parameters():
    print('이름은',name)
    print('파라미터는',param)
    print(param.requires_grad)

for name, param in model.bert.named_parameters():
    param.requires_grad = False

for name, param in model.bert.named_parameters():
    print('이름은',name)
    print('파라미터는',param)
    print(param.requires_grad)

for name, param in model.bert.named_parameters():
    if ( not name.startswith('pooler') ) and "layer.23" not in name :
        param.requires_grad = False

for name, param in model.bert.named_parameters():
    print('=====================')
    print('이름은',name)
    print('파라미터는',param)
    print(param.requires_grad)

for n,p in model.bert.named_parameters():
  print(n)
  #print("layer.23" not in name)

"""예제는, 여기서 풀러층과 마지막 레이어를 고정시켰음"""

model=BertForSequenceClassification.from_pretrained(kb_albert_model_path)

model

for n,p in model.bert.named_parameters():
  print(n)
  #print("layer.23" not in name)

for name, param in model.bert.named_parameters():
    print('=====================')
    print('이름은',name)
    print('파라미터는',param)
    print(param.requires_grad)

#여기 미세조정 파트!! 중요함!! 정확도를 결정해줌

'''
 각 레이어 내에서 파라미터가 있고, 레이어마다 .param 함수를 호출해줌으로써 이를 얻을 수 있다. 
 모든 파라미터는 requires_grad라는 특정성을 가진다. 이 특성의 기본값은 True인데, 역전파를 하겠다는 말이다. 
 그래서 한 레이어를 얼리려면 requires_grad를 False로 설정할 필요가 있다.
'''

#풀러층과 마지막 레이어 제외 고정
for name, param in model.bert.named_parameters():
    if ( not name.startswith('pooler') ) and "layer.11" not in name :
        param.requires_grad = False

# 풀러층빼고 고정
for name, param in model.bert.named_parameters():
    if not name.startswith('pooler'):
        param.requires_grad = False

for name, param in model.bert.named_parameters():
    print('=====================')
    print('이름은',name)
    print('파라미터는',param)
    print(param.requires_grad)

model=model.to(device) #모델 cuda에 올리기

from sklearn.metrics import accuracy_score,precision_recall_fscore_support
from transformers import TrainingArguments,Trainer

def compute_metrics(pred):
  labels=pred.label_ids
  preds=pred.predictions.argmax(-1)
  precision,recall,f1,_=precision_recall_fscore_support(labels,preds,average='binary')
  acc=accuracy_score(labels,preds)
  return {
      'accuracy':acc,
      'f1':f1,
      'precision':precision,
      'recall':recall
  }

training_args=TrainingArguments(
    output_dir='/content/gdriv/My Drive/Colab Notebooks/KB-albert/kb-albert-char-base-v2/nsmc_outputs_ex2',
    num_train_epochs=200,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.00005,
    save_strategy='epoch',
    evaluation_strategy='steps'
)

trainer=Trainer(
    model=model,
    args=training_args,
    train_dataset=train_set_dataset,
    eval_dataset=valid_set_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

"""#체크포인트 로드 및 예측"""

model=BertForSequenceClassification.from_pretrained('/content/gdriv/My Drive/Colab Notebooks/KB-albert/kb-albert-char-base-v2/nsmc_outputs_ex2/checkpoint-1600')

test_set_dataset=TheDataset(
    reviews=test_set.review.tolist(),
    sentiments=test_set.label.tolist(),
    tokenizer=tokenizer,
)

training_args=TrainingArguments(
    output_dir='/content/gdriv/My Drive/Colab Notebooks/KB-albert/kb-albert-char-base-v2/nsmc_outputs_ex2',
    do_predict=True
)

trainer=Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
)

trainer.predict(test_set_dataset)

